{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad36c014",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn (sklearn)\n",
    "\n",
    "This notebook demonstrates some of the most useful functions of the beautiful Scikit-Learn library.\n",
    "\n",
    "What we're going to cover:\n",
    "\n",
    "0. An end-to-end scikit-learn workflow\n",
    "1. Get the data ready to be used in machine learning models\n",
    "2. Choose the right estimator/algorithm for our problems\n",
    "3. Fit the model/algorith and use it to make predictions on our data\n",
    "4. Evaluating the model \n",
    "5. Improve the model\n",
    "6. Save and reload your trained model\n",
    "7. Putting it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d849981",
   "metadata": {},
   "source": [
    "Usefull links\n",
    "* [scikit-learn class video notes](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/introduction-to-scikit-learn-video.ipynb)\n",
    "* [scikit-learn condensed class notes](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/scikit-learn-what-were-covering.ipynb)\n",
    "* [Quick Machine Learning Overview](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb)\n",
    "* [scikit-learn workflow example](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/scikit-learn-workflow-example.ipynb)\n",
    "* [scikit-Learn Documentaion](https://scikit-learn.org/stable/user_guide.html)\n",
    "* [Algorithm Cheat Sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering = [\"0. An end-to-end scikit-learn workflow\",\n",
    "\"1. Get the data ready to be used in machine learning models\",\n",
    "\"2. Choose the right estimator/algorithm for our problems\",\n",
    "\"3. Fit the model/algorith and use it to make preedictions on our data\",\n",
    "\"4. Evaluating the model\",\n",
    "\"5. Improve the model\",\n",
    "\"6. Save and reload your trained model\",\n",
    "\"7. Putting it all together!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464eba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plty\n",
    "%matplotlib inline2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204cc05",
   "metadata": {},
   "source": [
    "## 0. An end-to-end Scikit-Learn Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the data ready\n",
    "heart_disease = pd.read_csv(\"../data/heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, the feature data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Create y (labels)\n",
    "y = heart_disease['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Choose the right model and hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# we'll keep the default hyperparameters\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc643540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit the model to the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025537c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make a prediction\n",
    "\n",
    "# y_label = clf.predict(np.array([0,2,3,4]))\n",
    "\n",
    "# The above doesn't work because it is not the correct shape.\n",
    "\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63621ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f46049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model on the training data and test data\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97635346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4cf89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improve a model\n",
    "# Try different amount of n_estimators\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(10, 100, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    clf = RandomForestClassifier(n_estimators = i).fit(X_train, y_train)\n",
    "    print(f\"Model accuracy on test set: {clf.score(X_test, y_test)* 100:.2f}%\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save a model and load it\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf, open(\"random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"random_forest_model_1.pkl\", \"rb\"))\n",
    "loaded_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714e549",
   "metadata": {},
   "source": [
    "## Optional: Debugging Warnings in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff3cf6",
   "metadata": {},
   "source": [
    "## 1. Getting our data ready to be used with machine learning\n",
    "\n",
    "###### Three main things we need to do to get the data ready:\n",
    "    1. Split the data into features and labels (usually, 'X' and y)\n",
    "    2. Filling also called imputing or disregarding missing values\n",
    "    3. Converting non-numerical values to numerical values aka feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease.drop('target', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a62b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart_disease['target']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a39ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(X):\", len(X))\n",
    "print(\"len(y):\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e076ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0] *0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final notes\n",
    "# Getting your Data Ready:\n",
    "# Clean Data -> Transform Data -> Reduce Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8f919",
   "metadata": {},
   "source": [
    "#### 1.1  Make sure it's all numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db46920",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = pd.read_csv(\"../data/car-sales-extended.csv\")\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y\n",
    "X = car_sales.drop('Price', axis=1)\n",
    "y = car_sales['Price']\n",
    "\n",
    "# Split into training and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb91a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "\"\"\"\n",
    "This section isn't going to work because the strings aren't able to go into the model\n",
    "I have commented it out so that I can rerun the entire notebook.\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d038e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the categories into numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = ['Make', \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder='passthrough')\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0489b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option to encode variables is pd.get_dummies()\n",
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\", \"Doors\"]])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36834836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refit the model\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01ecab",
   "metadata": {},
   "source": [
    "### 1.2 What if there were missing values?\n",
    "\n",
    "1. Fill them with some value (imputation)\n",
    "2. Remove the samples with missing data altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3246b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import car sales missing data\n",
    "# this data set has some missing values\n",
    "car_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6096cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y\n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try and convert our data to numbers\n",
    "# Turn the categories into numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = ['Make', \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder='passthrough')\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885303e",
   "metadata": {},
   "source": [
    "#### Option 1: Fill missing data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c877c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all of the columns except for the missing price, which we will delete later.\n",
    "\n",
    "# Fill the \"Make\" column\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Colour\" column\n",
    "car_sales_missing['Colour'].fillna(\"missing\", inplace = True)\n",
    "\n",
    "# fill the \"Odometer (KM)\" column\n",
    "car_sales_missing['Odometer (KM)'].fillna(car_sales_missing[\"Odometer (KM)\"].mean(), inplace = True)\n",
    "\n",
    "# fill the \"Doors\" column\n",
    "car_sales_missing[\"Doors\"].fillna(4, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our dataframe again\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing price value\n",
    "car_sales_missing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0105b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = ['Make', \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder='passthrough')\n",
    "\n",
    "transformed_X = transformer.fit_transform(car_sales_missing)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797c040",
   "metadata": {},
   "source": [
    "#### Option 2: Fill missing values with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcec59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with no labels\n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = car_sales_missing.drop(\"Price\", axis = 1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with Scikit-Learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Fill categorical with 'missing', and numerical values with the column mean...\n",
    "\n",
    "\"\"\"\n",
    "Normally, we would first need to split our data. We don't want to include the mean values of the test data. \n",
    "We want the replacement mean value to include only training data. Including the test data will\n",
    "influence the results of testing the test data.\n",
    "\"\"\"\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value = \"missing\" )\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)\n",
    "num_imputer= SimpleImputer(strategy='mean')\n",
    "\n",
    "# Define Columns\n",
    "cat_features = [\"Make\", \"Colour\"]\n",
    "door_features = [\"Doors\"]\n",
    "num_features = [\"Odometer (KM)\"]\n",
    "\n",
    "# Create an imputer (something that fills missing data)\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features), \n",
    "    (\"door_imputer\", door_imputer, door_features),\n",
    "    (\"num_imputer\", num_imputer, num_features)\n",
    "])\n",
    "\n",
    "# Transform the data\n",
    "filled_X = imputer.fit_transform(X)\n",
    "filled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled = pd.DataFrame(filled_X, columns = [\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "car_sales_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ead09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the categories into numbers\n",
    "# using the one  OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([ (\"one_hot\", one_hot, categorical_features) ] ,remainder=\"passthrough\")\n",
    "transformed_X = transformer.fit_transform(car_sales_filled)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588195be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got our data as numbers and filled (no missing values)\n",
    "# Let's fit a model\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model has done slightly worse.\n",
    "len(car_sales_filled), len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Most DataSets wont be in a form ready to use models.\n",
    "Some take more prep than others.\n",
    "Ususally you data:\n",
    "    - must be numberical\n",
    "        * categorical data needs to be encoded\n",
    "        * numberical data should also be scaled\n",
    "    - can not have missing values\n",
    "        * need to fill missing values with imputation\n",
    "        * The type of data determines the type of value replcement\n",
    "    \"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69497c",
   "metadata": {},
   "source": [
    "## 2. Choosing The Right Model For Your Data 2 (Regression)\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "* Sklearn refers to machine learning models/algorithms as estimators\n",
    "* Classification problem = predicting a category (heart disease or not)\n",
    "    * Sometimes you'll see `clf` (classifier) used as a classification estimator\n",
    "* Regression problem = predicting a number (selling price of a car)\n",
    "\n",
    "\n",
    "* [algorithm cheat-sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "* [sklearn built in datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 Picking a machine learning model for a regression problem.\n",
    "\n",
    "# Let's use the California Housing dataset\n",
    "# Get California Housing dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397852d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['target'] = housing['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20303d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_df = housing_df.drop(\"MedHouseVal\", inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f487e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Algorithm\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Set up random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model (on the training set)\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.score returns R^2 which is to say \"How well do these features predict the target variable?\"\n",
    "# R^2 is known as the coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW Pick a different model and try it out\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "        \n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a3d19",
   "metadata": {},
   "source": [
    "What if 'Ridge' didn't work or the score didn't fit our needs?\n",
    "\n",
    "We could always try a different model...\n",
    "\n",
    "How about we try an ensemble model (an ensemble is a combination of smaller models to try and make better predictions other than just a single model)\n",
    "\n",
    "Sklearn's Emsemble Models can be found here: [Ensemble Models](https://scikit-learn.org/stable/modules/ensemble.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df['target']\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "\n",
    "# Create the random forest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Score the Model\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997dc713",
   "metadata": {},
   "source": [
    "## 2.2 Picking a machine learning model for a classification problem\n",
    "\n",
    "let's go to the map: [machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some data for a classification problem.\n",
    "heart_disease = pd.read_csv(\"../data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3833c84",
   "metadata": {},
   "source": [
    "Consulting the map and it says to try `LinearSVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Linear SVC estimaor class\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "# setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# instantiate linearSVC\n",
    "clf = LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LinearSVC\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# instantiate Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest Classifier\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8de2e",
   "metadata": {},
   "source": [
    "Tidbit:\n",
    "    \n",
    "    1. If you have structured data, use ensemble methods\n",
    "    2. If you have have unstructured data, use deep learning or transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cde1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061da94",
   "metadata": {},
   "source": [
    "## 3 Fit the model/algorithm and our data and use it to make predictions\n",
    "\n",
    "### 3.1 Fitting the model to the data\n",
    "\n",
    "Different names for:\n",
    "* `X` = features,feature variables, data\n",
    "* `y` = labels, targets, target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# instantiate Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the data (training the machine learning model)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest Classifier (use the patterns the model has learned)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f77969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416aad7a",
   "metadata": {},
   "source": [
    "### 3.2 Make predictions using a machine learning model\n",
    "\n",
    "2 ways to make predictions:\n",
    "1. `predict()`\n",
    "2. `predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1de20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to make predictions\n",
    "# clf.predict(np.array([1,7,8,3,4])) # This doesn't work because it doesn't have the shape of our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to truth labels to evaluate the model\n",
    "y_preds = clf.predict(X_test)\n",
    "np.mean(y_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58845b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24ed41",
   "metadata": {},
   "source": [
    "Make predictions with `predict_proba()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b14874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba() returns probabilities of a classification label\n",
    "clf.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"n = [[0.89, 0.11] -> 0,\n",
    "[0.49, 0.51] -> 1,\n",
    "[0.43, 0.57] -> 1,\n",
    "[0.84, 0.16] -> 0,\n",
    "[0.18, 0.82] -> 1]\n",
    "for each array, there are two values.\n",
    "The first value is the probability of label 0, the second is the probability of label 1.\n",
    "[0.89, 0.11]\n",
    "[0, 1, 1, 0, 1]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict() on the same data\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fcd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888f8a4",
   "metadata": {},
   "source": [
    "`predict()` can also be used for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis = 1)\n",
    "y = housing_df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Create model instance\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "#Fit the model to the data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make our prediction\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predictions to the truth.\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa48289",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802129d",
   "metadata": {},
   "source": [
    "## 4. Evaluating A Machine Learning Model\n",
    "\n",
    "There are 3 ways to evaluate Scikit-Learn models/estimators:\n",
    "1. Estimator's built-in `score()` method\n",
    "2. The `scoring` parameter\n",
    "3. Problem specific metric functions\n",
    "   \n",
    "you can read more about these here: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230e60b",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating a model with the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e656927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Create train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create classifier model instance\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit classifier to training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91861211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest value for the score method is 1.0\n",
    "# The lowest is 0.0\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296e1cf",
   "metadata": {},
   "source": [
    "Let's use the `score` method on our regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split the data into X & y\n",
    "X = housing_df.drop(\"target\", axis = 1)\n",
    "y = housing_df['target']\n",
    "\n",
    "# split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2)\n",
    "\n",
    "# instantiate the model\n",
    "reg = RandomForestRegressor(n_estimators=500)\n",
    "\n",
    "# fit the model\n",
    "reg.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "# score the model\n",
    "reg.score(X_train, y_train), reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284740ab",
   "metadata": {},
   "source": [
    "### 4.2 Evaluating a Model using the `scoring` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Create train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create classifier model instance\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit classifier to training data\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ba8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c794ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#single training and test split score\n",
    "clf_single_score = clf.score(X_test, y_test)\n",
    "\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, y))\n",
    "\n",
    "# Compare the two\n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scoring parameter of classifier = mean accuracy\n",
    "# clf.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3aa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring parameter set to None by default\n",
    "cross_val_score(clf, X, y, scoring = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e43ee",
   "metadata": {},
   "source": [
    "What is the classifier default score?\n",
    " - Mean accuracy (number of correct predictions)/(total number of predictions) from 0 -> 1\n",
    " \n",
    "What is the regression defalut score?\n",
    "- Coefficient of determination aka R^2 - How well the features explain the target from 0 -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf6842",
   "metadata": {},
   "source": [
    "### 4.2.1 Classification model evaluation metrics\n",
    "\n",
    "1. Accuracy\n",
    "2. Area under ROC curve\n",
    "3. Confusion matrix\n",
    "4. Classification Report\n",
    "\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c978499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop('target', axis = 1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "cross_val_score = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier Cross-Validated Accuracy: {np.mean(cross_val_score) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01725a",
   "metadata": {},
   "source": [
    "**Area under the receiver operating characteristic curve (AUC/ROC)**\n",
    "\n",
    "* Area under curve (AUC)\n",
    "* ROC curve\n",
    "\n",
    "ROC curves are a comparison of a model's true positive rate (tpr)  versus a models false positive rate (fpr).\n",
    "\n",
    "\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c507f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with proabilities\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "y_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95065dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive = y_probs[:,1]\n",
    "y_probs_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fpr, tpr and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "# Check the false positive rates\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for plotting ROC curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr,tpr):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve given the false positive rate (fpr)\n",
    "    and the true postive rate (tpr) of a model.\n",
    "    \"\"\"\n",
    "    # Plot roc curve\n",
    "    plt.plot(fpr, tpr, color=\"orange\", label = \"ROC\")\n",
    "    # Plot line with no predictive power (baseline)\n",
    "    plt.plot([0,1], [0,1],color = \"darkblue\", linestyle=\"--\", label=\"Guessing\")\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel(\"False positive rate (fpr)\")\n",
    "    plt.ylabel(\"True positive rate (tpr)\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_probs_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perfect ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca55df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ca736",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.\n",
    "\n",
    "In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5650e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37222669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with pd.crosstab()\n",
    "# pd.crosstab(y_test, y_preds, rownames= [\"Actual Label\"], colnames=[\"Predicted\"])\n",
    "pd.crosstab(y_test,\n",
    "           y_preds,\n",
    "           rownames=[\"Acutal\"],\n",
    "           colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f34d5",
   "metadata": {},
   "source": [
    "|  |Predicted 0 | Predicted 1 |\n",
    "|----------- | ----------- | ----------- |\n",
    "|**Actual 0** | True Negatives | False Positives  |\n",
    "|**Actual 1** | False Negatives | True Positives |\n",
    "\n",
    "\n",
    "True Positive = model predicts 1 when truth is 1\n",
    "\n",
    "False Positive = model predicts 1 when truth is 0\n",
    "\n",
    "True Negative = model predicts 0 when truth is 0\n",
    "\n",
    "False Negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb43bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "23 + 6 + 6 + 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b448f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to install a conda package into the current environment from a Jupyter notebook\n",
    "\"\"\"import sys\n",
    "! conda install --yes --prefix {sys.prefix} seaborn\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de38c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43665629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our confusion matrix more visual with seasborn's heatmap()\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the font scale\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "#Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "# Plot is using Seaborn\n",
    "sns.heatmap(conf_mat);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d280a",
   "metadata": {},
   "source": [
    "### Creating a confusion matrix using scikit-learn\n",
    "\n",
    "To use the new methods of creating a confusion matrix with Scikit-Learn you will need sklearn version 1.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca73988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ab136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea64ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9667b5f",
   "metadata": {},
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dfa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b5874",
   "metadata": {},
   "source": [
    "**Precision**\n",
    ": Indicates the proportion of positive identification (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "\n",
    "**Recall**\n",
    ": Indicates the portion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "\n",
    "**F1 score** \n",
    ": A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "\n",
    "**Support** \n",
    ": The number of samples each metric was calculated on.\n",
    "\n",
    "**Accuracy** \n",
    ": The accuracy of the model in decimal form. Perfect accuracy is eqal to 1.0.\n",
    "\n",
    "**Macro avg** \n",
    ": Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn't class imbalance into account, so if you do have class imbalances, pay attention to this metric\n",
    "\n",
    "**Weighted avg** \n",
    ": Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. will give a high value when one class out performs another due to having more samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where precision and recall become valuable.\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 #only one positive case\n",
    "\n",
    "disease_preds = np.zeros(10000) # model predicts every case as 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true, disease_preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6661b2",
   "metadata": {},
   "source": [
    "To summarize classification metrics:\n",
    "\n",
    "* **Accuracy** is a good measure to start with if all classes are balanced( e.g. same amount of samples)\n",
    "* **Precision** and **Recall** become more important when classes are imbalanced.\n",
    "* If false positive predictions are worse than false negatives, aim for higher **precision**.\n",
    "* If false negative predictions are worse than false positives, aim for higher **recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9e1b2",
   "metadata": {},
   "source": [
    "### 4.2.2 Regression model evaluation metrics\n",
    "\n",
    "[Regression Model evaluation metrics documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "The ones we're going to cover are:\n",
    "1. R^2 The portion of the variation in the dependent variable that is predictable from the independent variables.\n",
    "2. Mean Absolute Error (MAE)\n",
    "3. Mean Squared Error (MSE)\n",
    "\n",
    "**R^2**\n",
    "What R-squared does: Compares your models predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, it's R^2 would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29030386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# fill an array with y_test mean\n",
    "y_test_mean = np.full(len(y_test), y_test.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21268f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f395d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_test, y_pred=y_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_test, y_pred=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice MSE and MAE\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_true=y_test, y_pred = y_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_true=y_test, y_pred = y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ce0a9",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "MAE is the average of the absolute differences between predictions and actual values.\n",
    "\n",
    "It gives you an idea of how wrong your models predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c312712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eea308",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae906fe",
   "metadata": {},
   "source": [
    "**MAE** On average the y_pred value is plus or minus the y_test value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc77e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\": y_test,\n",
    "                       \"predicted values\": y_preds})\n",
    "df[\"differences\"] = df[\"predicted values\"] - df[\"actual values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE using formulas and differences\n",
    "np.abs(df[\"differences\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out the Mean Squared Error on my own.\n",
    "\n",
    "(df[\"differences\"]**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55eec8",
   "metadata": {},
   "source": [
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "MSE is the mean of the square of the errors between the predicted values and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67397b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"squared_differences\"] = np.square(df['differences'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE by Hand\n",
    "squared = np.square(df['differences'])\n",
    "squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error = df.copy()\n",
    "df_large_error.iloc[0][\"squared_differences\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error[\"squared_differences\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.iloc[1:100] = 20\n",
    "df_large_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error[\"squared_differences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7016c0",
   "metadata": {},
   "source": [
    "## Which regression metric should you use?\n",
    "\n",
    "* **R^2** is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generrally, the closer your **R^2** value is to 1.0 the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.\n",
    "* **MAE** gives a better indication of how far off each of your model's predictions are on average.\n",
    "* As for **MAE** or **MSE**, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are)\n",
    "    * Pay more attention of **MAE**: When being 10,000 off is _twice_ as bad as being 5,000 off.\n",
    "    * Pay more attention to **MSE**: When being 10,000 off is _more_ than twice as bad being 5,000 off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe50565",
   "metadata": {},
   "source": [
    "# Machine Learning Model Evaluation\n",
    "\n",
    "Evaluating the results of a machine learning model is as important as building one.\n",
    "\n",
    "But just like how different problems have different machine learning models, different machine learning models have different evaluation metrics.\n",
    "\n",
    "Below are some of the most important evaluation metrics you'll want to look into for classification and regression models.\n",
    "\n",
    "## Classification Model Evaluation Metrics/Techniques\n",
    "\n",
    "* **Accuracy** - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n",
    "* **Precision** - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "* **Recall** - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "* **F1 score** - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "* **Confusion matrix** - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagonal line).\n",
    "* **Cross-validation** - Splits your dataset into multiple parts and train and tests your model on each part then evaluates performance as an average.\n",
    "* **Classification report** - Sklearn has a built-in function called `classification_report()` which returns some of the main classification metrics such as precision, recall and f1-score.\n",
    "* **ROC Curve - Also known as [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a plot of true positive rate versus false-positive rate.\n",
    "* **Area Under Curve (AUC) Score** - The area underneath the ROC curve. A perfect model achieves an AUC score of 1.0.\n",
    "\n",
    "### Which Classification metric should you use?\n",
    "\n",
    "* **Accuracy** is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1).\n",
    "\n",
    "* **Precision** and **recall** become more important when classes are imbalanced.\n",
    "\n",
    "* If false-positive predictions are worse than false-negatives, aim for higher precision.\n",
    "\n",
    "* If false-negative predictions are worse than false-positives, aim for higher recall.\n",
    "\n",
    "* **F1-score** is a combination of precision and recall.\n",
    "\n",
    "* A confusion matrix is always a good way to visualize how a classification model is going.\n",
    "\n",
    "## Regression Model Evaluation Metrics/Techniques\n",
    "\n",
    "* [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) - Compares your model's predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1.\n",
    "\n",
    "* [Mean Absolute Error (MAE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were.\n",
    "\n",
    "* [Mean squared error (MSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors).\n",
    "\n",
    "### Which regression metric should you use?\n",
    "\n",
    "* **R2** is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your **R2** value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.\n",
    "\n",
    "* **MAE** gives a better indication of how far off each of your model's predictions are on average.\n",
    "\n",
    "* As for **MAE** or **MSE**, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).\n",
    "\n",
    "    * Pay more attention to MAE: When being 10,000 off is **twice** as bad as being 5,000 off.\n",
    "\n",
    "    * Pay more attention to MSE: When being 10,000 off is **more than twice** as bad as being 5,000 off.\n",
    "\n",
    "For more resources on evaluating a machine learning model, be sure to check out the following resources:\n",
    "\n",
    "* [Scikit-Learn documentation for metrics and scoring (quantifying the quality of predictions)](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "* [Beyond Accuracy: Precision and Recall by Will Koehrsen](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c)\n",
    "\n",
    "* [Stack Overflow answer describing MSE (mean squared error) and RSME (root mean squared error)](https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python/37861832#37861832)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77d49f",
   "metadata": {},
   "source": [
    "### 4.2.3 Finally using the `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034694f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Cross-validation accuracy\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=None) # if scoring = None, it defaults to accuracy\n",
    "print(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)* 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)* 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5559987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated precision\n",
    "print(f\"The cross-validated precision is: {np.mean(cv_precision)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated recall\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca78402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\n",
    "cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated f1 score\n",
    "print(f\"The cross-validated f1 is: {np.mean(cv_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the scoring parameter being used for a regression problem.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X, y, cv=3, scoring=None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validated default score (R^2)\n",
    "print(f\"The average R^2 score is: {cv_r2.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error or MAE\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_mae = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "cv_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c02cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Average MAE: {np.mean(cv_mae)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ad5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f24940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error or MSE\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_mse = cross_val_score(model, X, y, cv=5, scoring = 'neg_mean_squared_error')\n",
    "cv_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The average MSE: {np.mean(cv_mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c40f7",
   "metadata": {},
   "source": [
    "## 4.3 Using different evaluationmetrics with scikit-learn Functions\n",
    "\n",
    "The 3rd way to evaluate scikit-learn machine learning models/estimators is to use the `sklearn.metrics` module:\n",
    "[sklearn.etrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a Classification Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "# create X & y\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# Evaluate model using evaluation functions\n",
    "print(f\"\"\"--  Classifier metrics on the test set --\n",
    "accuracy score: {accuracy_score(y_true=y_test, y_pred=y_pred)*100:.2f}%\n",
    "precision score: {precision_score(y_true=y_test, y_pred=y_pred):.3f}%\n",
    "recall score: {recall_score(y_true=y_test, y_pred=y_pred):.3f}\n",
    "f1 score: {f1_score(y_true=y_test, y_pred=y_pred):.3f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "# create X & y\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create model\n",
    "reg = RandomForestRegressor()\n",
    "\n",
    "# Fit model\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model using evaluation functions\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(f\"\"\" ---  Regression Metrics  --- \n",
    "R2 score: {r2_score(y_test, y_pred):.3f}\n",
    "MAE: {mean_absolute_error(y_test, y_pred):.3f}\n",
    "MSE: {mean_squared_error(y_test, y_pred):.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d2808",
   "metadata": {},
   "source": [
    "## 5. Improving a model\n",
    "\n",
    "**first predictions** of a model are the **baseline predictions**\n",
    "\n",
    "**first model** created before improvements is called the **baseline model**.\n",
    "\n",
    "\n",
    "How do we improve our **baseline predictions**?\n",
    "\n",
    "* From a data perspective:\n",
    " * Could we collect more data? (generally, the more data, the better)\n",
    " * Could we improve our data?\n",
    "        \n",
    "* From a model Perspective:\n",
    " * Is there a better model we could use?\n",
    " * Could we improve the current model?\n",
    "    \n",
    "    \n",
    "Parameters vs Hyper Parameters\n",
    "   * Parameters = model find these patterns\n",
    "   * Hyper Parameters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters:\n",
    "1. By hand\n",
    "2. Randomly with RandomSearchCV\n",
    "3. Exhaustively with GridSearchVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b7d11",
   "metadata": {},
   "source": [
    "### 5.1 Tuning hyperprameters by hand\n",
    "\n",
    "For tuning by hand, we split our data set into three\n",
    "\n",
    "- **training split** (70%-80%) We train the model on this set\n",
    "- **validation split** (10%-15%) We tune the hyperparameters on this set\n",
    "- **test split** (10%-15%) We test the model on this set\n",
    "\n",
    "EX: 100 patient records\n",
    "- 70% training\n",
    "- 15% hyperparameter tuning\n",
    "- 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"Performs evaluation comparison on y_true labels vs y_pred labels\n",
    "    on a classification model.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                  \"precision\": round(precision, 2),\n",
    "                  \"recall\": round(recall, 2),\n",
    "                  \"f1\": round(f1, 2)}\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision: .2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ff8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# shuffle the data\n",
    "heart_disease_shuffled = heart_disease.sample(frac=1)\n",
    "\n",
    "# split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# split the data into train, validation, and test set\n",
    "train_split = round(0.7 * len(heart_disease_shuffled)) # 70 % of data\n",
    "valid_split = round(train_split + 0.15 * len(heart_disease_shuffled))\n",
    "X_train, y_train = X[:train_split] , y[:train_split]\n",
    "X_valid, y_valid = X[train_split: valid_split], y[train_split: valid_split]\n",
    "X_test, y_test = X[valid_split:], y[valid_split:]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make baseline Predictions\n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier on validation set\n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9672b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds_2 = clf_2.predict(X_valid)\n",
    "\n",
    "#Evaluate the 2nd classifier\n",
    "clf_2_metrics = evaluate_preds(y_valid, y_preds_2)\n",
    "clf_2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_3 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds_3 = clf_3.predict(X_valid)\n",
    "\n",
    "#Evaluate the 2nd classifier\n",
    "clf_3_metrics = evaluate_preds(y_valid, y_preds_3)\n",
    "clf_3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acef7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e267d8",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "       \"max_depth\": [None, 5, 10, 20, 30],\n",
    "       \"max_features\": [\"auto\", \"sqrt\"],\n",
    "       \"min_samples_split\": [2,4,6],\n",
    "       \"min_samples_leaf\": [1,2,4]}\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X and y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis =1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(estimator = clf, \n",
    "                            param_distributions=grid, \n",
    "                            n_iter=10, # number of models to try\n",
    "                            cv = 5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the RandomizedSearchCV of CLF\n",
    "rs_clf.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3376ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e39445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds = rs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics = evaluate_preds(y_test, rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b65cb5",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GridSearchCV method goes through each possbile \n",
    "# combination of hyperparameters for a model and then again for each fold of the cv\n",
    "# for grid 1 that is\n",
    "cv=5\n",
    "6 * 5 * 2 * 3 * 3 * cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c39cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's too many models so we're going to make the grid smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {'n_estimators': [100, 200, 500],\n",
    "          'max_depth': [None],\n",
    "          'max_features': ['auto', 'sqrt'],\n",
    "          'min_samples_split': [6],\n",
    "          'min_samples_leaf': [1, 2]\n",
    "         }\n",
    "\n",
    "# now we have\n",
    "3 * 1 * 2* 1 * 2 * cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f639e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X and y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis =1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator = clf, \n",
    "                            param_grid=grid_2, \n",
    "                            cv = 5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV of CLF\n",
    "gs_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6435e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "# evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded76c76",
   "metadata": {},
   "source": [
    "Let's compare our different models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,\n",
    "                              \"clf_2\": clf_2_metrics,\n",
    "                              \"random search\": rs_metrics,\n",
    "                              \"grid search\": gs_metrics})\n",
    "\n",
    "compare_metrics.plot.bar(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29abcc7",
   "metadata": {},
   "source": [
    "## 6.0 Saving and loading trained machine learning models\n",
    "\n",
    "Two ways to save and load machine learning models:\n",
    "1. With Python's `pickle` module\n",
    "2. With `joblib` module\n",
    "\n",
    "**Pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save an existing model to file\n",
    "pickle.dump(gs_clf, open(\"gs_random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "loaded_pickle_model = pickle.load(open(\"gs_random_forest_model_1.pkl\",\"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e48aaa",
   "metadata": {},
   "source": [
    "**Joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c425f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save model to file\n",
    "dump(gs_clf, filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a saved joblid model\n",
    "loaded_jobmodel = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb38542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and evaluate joblib predictions\n",
    "joblib_y_preds = loaded_jobmodel.predict(X_test)\n",
    "evaluate_preds(y_test, joblib_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fac62",
   "metadata": {},
   "source": [
    "### 7. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b5135",
   "metadata": {},
   "source": [
    "Steps we want to do (all in one cell):\n",
    "1. Fill the missing data\n",
    "2. Convert data to numbers\n",
    "3. Build a model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992791b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data ready\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# set up random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing labels\n",
    "data = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipeline\n",
    "categorical_features = ['Make', \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "door_features = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "# Setup the preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\"cat\", categorical_transformer, categorical_features),\n",
    "                        (\"door\", door_transformer, door_features),\n",
    "                        (\"num\", numeric_transformer, numeric_features)])\n",
    "\n",
    "# Creating a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdde2f3",
   "metadata": {},
   "source": [
    "It's also possible to use `GridsearchCV` or `RandomizedSearchCV` with our `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV with our regression Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"model__n_estimators\": [100, 1000], \n",
    "    \"model__max_depth\": [None, 5],\n",
    "    \"model__max_features\":[\"auto\"],\n",
    "    \"model__min_samples_split\": [2,4]\n",
    "}\n",
    "\n",
    "gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)\n",
    "gs_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588990f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
